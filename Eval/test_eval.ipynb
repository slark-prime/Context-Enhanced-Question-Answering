{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"eval_blocks.jsonl\", 'r') as f:\n",
    "    block_list = [json.loads(line)[\"text\"] for line in f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directly copy the text block to the clipboard and then manually paste to ChatGPT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "prompt = block_list[29] + \"Please generate four questions based on the provided text. The questions should represent different types, specifically factoid, list, polar (yes/no), and explanatory. Craft each question in a way that it corresponds to a single context within the text, reducing potential for multiple interpretations - use determiners where necessary for clarity. The answer should be locatable within the text. Provide the responses to these questions in an accurate and concise manner.  Format the output like this: {“question”: “The text of the question goes here”, “type”: “The type of the question goes here”, “answer”:”the text of the answer goes here”}\"\n",
    "# To copy text to the clipboard\n",
    "pyperclip.copy(prompt)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After get all the answers prepared on ChatGPT website interface. Download the HTML file by copying the outermost tag from Safari element inspector. Then extract the answers out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# Open the new HTML file and parse it with BeautifulSoup\n",
    "with open('test_extractor.html', 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the <p> tags\n",
    "p_tags = soup.find_all('p')\n",
    "# Regular expression to capture text within {}\n",
    "json_pattern = re.compile(r'\\{.*?\\}')\n",
    "\n",
    "# Open the results file in write mode ('w') to overwrite any existing data\n",
    "with open('questions.jsonl', 'w') as outfile:\n",
    "    # Iterate over each <p> tag\n",
    "    for tag in p_tags:\n",
    "        text = tag.get_text(strip=True)\n",
    "        # Find all potential JSON objects in the text\n",
    "        potential_jsons = json_pattern.findall(text)\n",
    "        for potential_json in potential_jsons:\n",
    "            # Try to parse the text as JSON\n",
    "            try:\n",
    "                data = json.loads(potential_json)\n",
    "                # If successful, write the data to the file as a JSON line\n",
    "                json.dump(data, outfile)\n",
    "                outfile.write('\\n')\n",
    "            except json.JSONDecodeError:\n",
    "                # If the text couldn't be parsed as JSON, ignore it\n",
    "                pass\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "add block id to the `questions.jsonl` file and store as `question_block.jsonl`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "counter = 0\n",
    "group_id = 1\n",
    "\n",
    "with open('questions.jsonl', 'r') as f, open('question_block.jsonl', 'w') as fout:\n",
    "    for  qid, line in enumerate(f, start=1):\n",
    "        # Remove newline characters and spaces\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check if line is not empty\n",
    "        if line:\n",
    "            # Load JSON object from line\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # Add the group id\n",
    "            obj['block_id'] = group_id\n",
    "            obj[\"question_id\"] = qid\n",
    "            # Write the updated JSON object to the output file\n",
    "            fout.write(json.dumps(obj) + '\\n')\n",
    "\n",
    "            # Increment counter\n",
    "            counter += 1\n",
    "\n",
    "            # If counter reaches 4, reset it and increment group id\n",
    "            if counter == 4:\n",
    "                counter = 0\n",
    "                group_id += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.mps.enabled = True\n",
    "torch.backends.mps.max_concurrency = 1\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "120"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(answers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_falcon-7b-instruct_instructor-xl.jsonl\n"
     ]
    }
   ],
   "source": [
    "file_name = \"answers_\" + myCAQA.llm_repo_id.split('/')[-1] + \"_\" + myCAQA.embedding_model.split('/')[-1]  + \".jsonl\"\n",
    "# Write to answers to json line file\n",
    "with open(file_name, 'w') as f:\n",
    "    for i, answer in enumerate(answers, start=1):\n",
    "        line = {\n",
    "            \"question_id\": i,\n",
    "            \"answer\": answer,\n",
    "            \"LLM\" : myCAQA.llm_repo_id,\n",
    "            \"embedding_model\" : myCAQA.embedding_model\n",
    "        }\n",
    "        f.write(json.dumps(line) + \"\\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"answers.jsonl\", 'r') as asw:\n",
    "    pass\n",
    "with open(\"questions.jsonl\", 'r') as qst:\n",
    "    for line in qst:\n",
    "        qs_dict = json.loads(line)\n",
    "        q = qs_dict[\"question\"]\n",
    "        block_id = qs_dict[]\n",
    "        json.loads(line)[\"block_id\"]\n",
    "with open(\"eval_blocks.json\", 'r') as blk:\n",
    "    for line in blk:\n",
    "        json.loads(line)[\"block_id\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_and_extract_fields(file_path, fields):\n",
    "    extracted_data = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            extracted_line_data = {field: data.get(field, None) for field in fields}\n",
    "            extracted_data.append(extracted_line_data)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def pair_questions_and_answers(question_file, answer_file):\n",
    "    # Extract 'question_id' and 'question' from the question file\n",
    "    question_data = read_jsonl_and_extract_fields(question_file, ['question_id', 'question', 'answer'])\n",
    "\n",
    "    # Convert the extracted data to a dictionary for easy lookup by 'question_id'\n",
    "    question_dict = {item['question_id']: [item['question'], item['answer']] for item in question_data}\n",
    "\n",
    "    # Extract 'question_id' and 'answer' from the answer file\n",
    "    answer_data = read_jsonl_and_extract_fields(answer_file, ['question_id', 'answer'])\n",
    "\n",
    "    # Pair the questions with the answers using 'question_id'\n",
    "    paired_data = []\n",
    "    for item in answer_data:\n",
    "        question_id = item['question_id']\n",
    "        question_and_reference = question_dict.get(question_id, None)\n",
    "        answer = item['answer']\n",
    "        paired_data.append({\"question\": question_and_reference[0],\n",
    "                            \"answer\": answer,\n",
    "                            \"reference_answer\": question_and_reference[1]})\n",
    "\n",
    "    return paired_data\n",
    "\n",
    "question_file_path = \"question_block.jsonl\"  # replace with your question file path\n",
    "answer_file_path = \"answers_vicuna-7b-v1.3_instructor-xl0.1.jsonl\"  # replace with your answer file path\n",
    "qa_pairs = pair_questions_and_answers(question_file_path, answer_file_path)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'Who has primary responsibility for risk management at Entergy?',\n 'answer': ' It depends on who is being referred to in the text. At Entergy Corporation, the board of directors has primary responsibility for risk management. However, within the utility companies and EWC, risk management responsibilities vary.',\n 'reference_answer': \"The board's audit committee has primary responsibility for risk management.\"}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "MAX_API_RETRY = 5\n",
    "import logging\n",
    "import openai\n",
    "import time\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_eval(sys_prompt, user_prompt: str, max_tokens: int):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    for i in range(MAX_API_RETRY):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_prompt,\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0,  # TODO: figure out which temperature is best for evaluation\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            content = response #[\"choices\"][0][\"message\"][\"content\"]\n",
    "            logger.info(content)\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            time.sleep(5)\n",
    "    logger.error(f\"Failed after {MAX_API_RETRY} retries.\")\n",
    "    return \"error\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "system_prompt = \"You are an evaluator assessing the quality of the AI assistant's answer based solely on the retrieved context.\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant's Answer]\n",
    "{answer}\n",
    "\n",
    "[The End of Assistant's Answer]\n",
    "\n",
    "[The Start of Reference Answer]\n",
    "{reference}\n",
    "\n",
    "[The End of Reference Answer]\n",
    "\n",
    "[System]\n",
    "{prompt}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to provide feedback on the performance of a single AI assistant in response to the user question displayed above. Please evaluate the assistant's response with the provided reference answer, which is entirely based on correctly retrieved information, in terms of its helpfulness, relevance, accuracy, and level of detail.\n",
    "When evaluating, please assume that the assistant has retrieved the necessary information to respond.\n",
    "The assistant's performance will be rated on a scale of 1 to 10, with a higher score indicating superior overall performance. Please initially output a single line containing only one value, indicating the score for the assistant.\n",
    "\n",
    "Subsequently, offer a concise explanation of your evaluation, ensuring impartiality and that the order in which the response and reference answer are presented does not affect your judgement.\n",
    "\"\"\"\n",
    "\n",
    "i = 0\n",
    "prompt = prompt_template.format(question=qa_pairs[i]['question'], answer=qa_pairs[i]['answer'], reference=qa_pairs[i]['reference_answer'], prompt=prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n[Question]\\nWho has primary responsibility for risk management at Entergy?\\n\\n[The Start of Assistant's Answer]\\n It depends on who is being referred to in the text. At Entergy Corporation, the board of directors has primary responsibility for risk management. However, within the utility companies and EWC, risk management responsibilities vary.\\n\\n[The End of Assistant's Answer]\\n\\n[The Start of Reference Answer]\\nThe board's audit committee has primary responsibility for risk management.\\n\\n[The End of Reference Answer]\\n\\n[System]\\n\\nYour task is to provide feedback on the performance of a single AI assistant in response to the user question displayed above. Please evaluate the assistant's response with the provided reference answer, which is entirely based on correctly retrieved information, in terms of its helpfulness, relevance, accuracy, and level of detail.\\nWhen evaluating, please assume that the assistant has retrieved the necessary information to respond.\\nThe assistant's performance will be rated on a scale of 1 to 10, with a higher score indicating superior overall performance. Please initially output a single line containing only one value, indicating the score for the assistant.\\n\\nSubsequently, offer a concise explanation of your evaluation, ensuring impartiality and that the order in which the response and reference answer are presented does not affect your judgement.\\n\\n\\n\""
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"8.5\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1689425067,\n",
      "  \"id\": \"chatcmpl-7cYvLfKGCnKDi7sDcut7XqwU60eHE\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 3,\n",
      "    \"prompt_tokens\": 253,\n",
      "    \"total_tokens\": 256\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "openai.api_key = \"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\"\n",
    "with get_openai_callback() as cb:  # calculate token usage if openai LLM is used\n",
    "    review = get_eval(system_prompt, prompt, 1024)\n",
    "    print(cb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
